{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Ignore warnings for a cleaner display\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:51:58.342440Z","iopub.execute_input":"2021-09-24T02:51:58.342869Z","iopub.status.idle":"2021-09-24T02:51:58.354627Z","shell.execute_reply.started":"2021-09-24T02:51:58.342773Z","shell.execute_reply":"2021-09-24T02:51:58.353582Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\nimport keras\nimport tensorflow as tf\nimport PIL\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.preprocessing import image","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:51:58.356254Z","iopub.execute_input":"2021-09-24T02:51:58.356840Z","iopub.status.idle":"2021-09-24T02:52:00.216376Z","shell.execute_reply.started":"2021-09-24T02:51:58.356802Z","shell.execute_reply":"2021-09-24T02:52:00.215548Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Install dependencies\n!pip install tf_explain --quiet","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:52:00.218227Z","iopub.execute_input":"2021-09-24T02:52:00.218570Z","iopub.status.idle":"2021-09-24T02:52:06.741153Z","shell.execute_reply.started":"2021-09-24T02:52:00.218534Z","shell.execute_reply":"2021-09-24T02:52:06.740230Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tf_explain.core.grad_cam import GradCAM","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:52:06.744810Z","iopub.execute_input":"2021-09-24T02:52:06.745097Z","iopub.status.idle":"2021-09-24T02:52:06.770536Z","shell.execute_reply.started":"2021-09-24T02:52:06.745066Z","shell.execute_reply":"2021-09-24T02:52:06.769758Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Wrangling <a name='1'></a>\n\n## 1.1 Data Loading <a name='1.1'></a>\n\n**Data Source:** \nBut we are currently going to use only the first 45,000 images  as a start then update the model step-by-step due to the size of the data (1 GB). \n\n**The facial keypoints dataset:** this dataset contains the x- and y-coordinates of the facial images' five keypoints (left eye, right eye, nose, left side of the mouth, and right side of the mouth).\n\nAlthough the original dataset also includes 40 binary values of facial attributes, I will be focusing on the facial keypoints only for now.\n\n### 1.1.1 Loading Keypoints Dataset <a name='1.1.1'></a>","metadata":{}},{"cell_type":"code","source":"#Define paths to keypoints datasets\nkeypts_data_path = \"../input/celeba-dataset/list_landmarks_align_celeba.csv\"\nimages_data_path = \"../input/celeba-dataset/img_align_celeba/img_align_celeba\"\n\n#Select only 35k images first\nimg_data_size = 35000\n\n#Original image dimensions\nx_og = 178\ny_og = 218\n\n# New image dimensions\nx_ = 80\nimage_size_ratio = x_og / y_og\ny_ = int(image_size_ratio * x_)\n\n# Image Sizes\noriginal_image_size = (x_og, y_og)\nnew_image_size = (x_,y_)\n\n# The image size that will be used in the training process\nimage_size_training = new_image_size","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:52:06.771780Z","iopub.execute_input":"2021-09-24T02:52:06.772167Z","iopub.status.idle":"2021-09-24T02:52:06.779197Z","shell.execute_reply.started":"2021-09-24T02:52:06.772131Z","shell.execute_reply":"2021-09-24T02:52:06.777780Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"keypts_og = pd.read_csv(keypts_data_path)[:img_data_size]\nkeypts_og.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:52:06.780499Z","iopub.execute_input":"2021-09-24T02:52:06.781004Z","iopub.status.idle":"2021-09-24T02:52:07.034199Z","shell.execute_reply.started":"2021-09-24T02:52:06.780954Z","shell.execute_reply":"2021-09-24T02:52:07.033207Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"keypts_og.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:52:07.035625Z","iopub.execute_input":"2021-09-24T02:52:07.035984Z","iopub.status.idle":"2021-09-24T02:52:07.082249Z","shell.execute_reply.started":"2021-09-24T02:52:07.035946Z","shell.execute_reply":"2021-09-24T02:52:07.081359Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"keypts_og.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:52:07.083566Z","iopub.execute_input":"2021-09-24T02:52:07.083904Z","iopub.status.idle":"2021-09-24T02:52:07.101865Z","shell.execute_reply.started":"2021-09-24T02:52:07.083869Z","shell.execute_reply":"2021-09-24T02:52:07.100886Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"keypts_og.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:52:07.104868Z","iopub.execute_input":"2021-09-24T02:52:07.105227Z","iopub.status.idle":"2021-09-24T02:52:07.111534Z","shell.execute_reply.started":"2021-09-24T02:52:07.105190Z","shell.execute_reply":"2021-09-24T02:52:07.110343Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2 Images Data\nLoading the images data involves converting it to arrays so it may be used it in training the model.","metadata":{}},{"cell_type":"code","source":"images = []\n\nfor idx in range(keypts_og.shape[0]):\n    #Retrieve path\n    path = \"{}/{}\".format(str(images_data_path), str(keypts_og.iloc[idx].image_id))\n    \n    #Reading images\n    image = PIL.Image.open(path).resize(image_size_training)\n    image_array = np.asarray(image) / 255\n    \n    #Append images\n    images.append(image_array)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:52:07.113839Z","iopub.execute_input":"2021-09-24T02:52:07.114486Z","iopub.status.idle":"2021-09-24T02:53:37.225338Z","shell.execute_reply.started":"2021-09-24T02:52:07.114444Z","shell.execute_reply":"2021-09-24T02:53:37.224440Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Convert to array\nimages = np.array(images)\n\nimages.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:37.226678Z","iopub.execute_input":"2021-09-24T02:53:37.227034Z","iopub.status.idle":"2021-09-24T02:53:38.567959Z","shell.execute_reply.started":"2021-09-24T02:53:37.226974Z","shell.execute_reply":"2021-09-24T02:53:38.566975Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Plotting sample images\nidx1, idx2, idx3, idx4 = np.random.randint(0, img_data_size, size=4)\n\nfig, axs = plt.subplots(2, 2)\naxs[0,0].imshow(images[idx1])\naxs[1,0].imshow(images[idx2])\naxs[1,1].imshow(images[idx3])\naxs[0,1].imshow(images[idx4])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:38.569384Z","iopub.execute_input":"2021-09-24T02:53:38.569905Z","iopub.status.idle":"2021-09-24T02:53:39.104430Z","shell.execute_reply.started":"2021-09-24T02:53:38.569861Z","shell.execute_reply":"2021-09-24T02:53:39.103549Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Data Cleaning <a name='1.2'></a>\nThere isn't much to clean in the data since I would like the deep learning to obtain as much of the raw data as possible. However, I will check for any missing values.","metadata":{}},{"cell_type":"code","source":"#Check for missing values in keypoints dataset\nkeypts_og.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:39.105607Z","iopub.execute_input":"2021-09-24T02:53:39.105953Z","iopub.status.idle":"2021-09-24T02:53:39.119114Z","shell.execute_reply.started":"2021-09-24T02:53:39.105918Z","shell.execute_reply":"2021-09-24T02:53:39.117935Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 2. EDA\nIn order to take an initial closer look at the dataset, I want to take a look at the following:\n1. Distribution of the keypoints coordinates\n2. Heatmap of keypoint coordinates\n3. Any correlations\n\n## 2.1 Distribution of the Keypoints Coordinates <a name='2.1'></a>\nSince the images dataset is already cropped into nice images with the face roughly centered, I would expect the coordinates to be roughly concentrated around particular areas; for example, the right eye coordinates would be expected to mostly be in the right upper quadrant. If this is not the case, there could be transformed or different types of facial images in the dataset.","metadata":{}},{"cell_type":"code","source":"#Plotting distribution of the keypoints' coordinates\nfig, axs = plt.subplots(5, 2, figsize=(13,10))\n\n#Left eye\naxs[0,0].hist(keypts_og.lefteye_x, bins=15)\naxs[0,0].set_title('Left Eye X-coord')\naxs[0,1].hist(keypts_og.lefteye_y, bins=15)\naxs[0,1].set_title('Left Eye Y-coord')\n\n#Right eye\naxs[1,0].hist(keypts_og.righteye_x, bins=15)\naxs[1,0].set_title('Right Eye X-coord')\naxs[1,1].hist(keypts_og.righteye_y, bins=15)\naxs[1,1].set_title('Right Eye Y-coord')\n\n#Nose\naxs[2,0].hist(keypts_og.nose_x, bins=15)\naxs[2,0].set_title('Nose X-coord')\naxs[2,1].hist(keypts_og.nose_y, bins=15)\naxs[2,1].set_title('Nose Y-coord')\n\n#Left mouth\naxs[3,0].hist(keypts_og.leftmouth_x, bins=15)\naxs[3,0].set_title('Left Mouth X-coord')\naxs[3,1].hist(keypts_og.leftmouth_y, bins=15)\naxs[3,1].set_title('Left Mouth Y-coord')\n\n#Right mouth\naxs[4,0].hist(keypts_og.rightmouth_x, bins=15)\naxs[4,0].set_title('Right Mouth X-coord')\naxs[4,1].hist(keypts_og.rightmouth_y, bins=15)\naxs[4,1].set_title('Right Mouth Y-coord')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:39.120931Z","iopub.execute_input":"2021-09-24T02:53:39.121472Z","iopub.status.idle":"2021-09-24T02:53:41.132305Z","shell.execute_reply.started":"2021-09-24T02:53:39.121433Z","shell.execute_reply":"2021-09-24T02:53:41.131420Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Heatmap of General Facial Coordinates<a name='2.2'></a>","metadata":{}},{"cell_type":"code","source":"#Plot a sample box of with a heatmap of where the coordinates are\nfig, ax = plt.subplots(figsize=(6,6*image_size_ratio))\n\nrect = patches.Rectangle((0, 0), 178, -218, linewidth=4, facecolor='none')\nax.add_patch(rect)\n\n#Add scatter plots of the coordinates with colorblind friendly colors\nax.scatter(keypts_og.righteye_x, -keypts_og.righteye_y, alpha=0.05, color='#648FFF', label='Right Eye')\nax.scatter(keypts_og.lefteye_x, -keypts_og.lefteye_y, alpha=0.05, color='#785EF0', label='Left Eye')\nax.scatter(keypts_og.nose_x, -keypts_og.nose_y, alpha=0.05, color='#DC267F', label='Nose')\nax.scatter(keypts_og.rightmouth_x, -keypts_og.rightmouth_y, alpha=0.05, color='#FE6100', label='Right Mouth')\nax.scatter(keypts_og.leftmouth_x, -keypts_og.leftmouth_y, alpha=0.05, color='#FFB000', label='Left Mouth')\n\n#Add legend for the colors\nleg = ax.legend()\n#Set legend opacity colors to opaque (1)\nfor lh in leg.legendHandles:\n    lh.set_alpha(1)\n\n#Display positive y-ticks\nax.get_yaxis().set_ticklabels([250, 200, 150, 100, 50, 0])\n    \nplt.title('Heatmap Distribution of Facial Keypoints')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:41.136124Z","iopub.execute_input":"2021-09-24T02:53:41.138170Z","iopub.status.idle":"2021-09-24T02:53:48.004268Z","shell.execute_reply.started":"2021-09-24T02:53:41.138126Z","shell.execute_reply":"2021-09-24T02:53:48.003457Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Correlation between Coordinates <a name='2.3'></a>\nI expect there to be some correlation since there is a general relationship between the facial keypoints. For example, the two eyes can only be so far apart, and yet not too close together. At the same time, this rationale would only work for front-facing facial images; the distances would be different in a facial image at an angle.","metadata":{}},{"cell_type":"code","source":"#Correlation map\ncorrmap = keypts_og.iloc[:,1:].corr()\n\n#Plot heatmap of attributes\nplt.subplots(figsize=(10, 8))\nsns.heatmap(corrmap, cmap='RdYlBu', vmin=-1, vmax=1, annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:48.005583Z","iopub.execute_input":"2021-09-24T02:53:48.005938Z","iopub.status.idle":"2021-09-24T02:53:48.698456Z","shell.execute_reply.started":"2021-09-24T02:53:48.005900Z","shell.execute_reply":"2021-09-24T02:53:48.697668Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Some of these correlations are surprising. The nose coordinates don't have much correlation to any of the points. The eyes' coordinates all seem to be correlated, except for the left eye y-coordinate with the right eye's.","metadata":{}},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"#Read image based on index\ndef imread_index(index, size=image_size_training, path=images_data_path):\n    \"\"\"This function helps read images based on the its index by converting it to an array.\n    \"\"\"\n    path = \"{}/{}\".format(str(path), str(keypts_og.iloc[index].image_id))\n    \n    #read the image\n    image = PIL.Image.open(path).resize(size)\n    image_array = np.asarray(image)\n    \n    return image_array","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:48.699707Z","iopub.execute_input":"2021-09-24T02:53:48.700053Z","iopub.status.idle":"2021-09-24T02:53:48.705337Z","shell.execute_reply.started":"2021-09-24T02:53:48.700016Z","shell.execute_reply":"2021-09-24T02:53:48.704404Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Get a list of all key points of the face\ndef img_keypts_list(index, df=keypts_og):\n    \"\"\" This function returns a list of all the key points of the face image so \n    it's easier to plot\n    \"\"\"\n    points_list = [df.iloc[index].lefteye_x, df.iloc[index].lefteye_y,\n                  df.iloc[index].righteye_x, df.iloc[index].righteye_y,\n                  df.iloc[index].nose_x, df.iloc[index].nose_y,\n                  df.iloc[index].leftmouth_x, df.iloc[index].leftmouth_y,\n                  df.iloc[index].rightmouth_x, df.iloc[index].rightmouth_y]\n    \n    return points_list","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:48.706504Z","iopub.execute_input":"2021-09-24T02:53:48.706964Z","iopub.status.idle":"2021-09-24T02:53:48.716628Z","shell.execute_reply.started":"2021-09-24T02:53:48.706924Z","shell.execute_reply":"2021-09-24T02:53:48.715774Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Plot image with green bounding box and keypoints\ndef plot_face_bbox(index, df=keypts_og, size=original_image_size):\n    \"\"\" This function plots the face image with its keypoints and bounding box\n    \"\"\"\n    img = imread_index(index, size)\n    points_list = img_keypts_list(index, df)\n    \n    #Plotting the image\n    fig, ax = plt.subplots()\n    ax.imshow(img)\n    \n    #Plot the face points\n    ax.plot(points_list[::2], points_list[1::2], 'bo-')\n    \n    #Plot bounding box\n    width = abs(points_list[0] - points_list[8] - 60) #obtain width from left eye x to right mouth x\n    height = abs(points_list[1] - points_list[9] - 75)#obtain width from left eye y to right mouth y\n    rect = patches.Rectangle((points_list[0]-30, points_list[1]-40), width, height, linewidth=4, edgecolor='g', facecolor='none')\n    ax.add_patch(rect)\n    \n    #Remove axis\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:48.717854Z","iopub.execute_input":"2021-09-24T02:53:48.718250Z","iopub.status.idle":"2021-09-24T02:53:48.728943Z","shell.execute_reply.started":"2021-09-24T02:53:48.718211Z","shell.execute_reply":"2021-09-24T02:53:48.728229Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#test\nidx1, idx2, idx3 = np.random.randint(0, img_data_size, size=3)\n\n#Test plotting sample images\nplot_face_bbox(idx1)\nplot_face_bbox(idx2)\nplot_face_bbox(idx3)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:48.730187Z","iopub.execute_input":"2021-09-24T02:53:48.730779Z","iopub.status.idle":"2021-09-24T02:53:48.973188Z","shell.execute_reply.started":"2021-09-24T02:53:48.730742Z","shell.execute_reply":"2021-09-24T02:53:48.972202Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Since the image sizes were scaled down, the key points also need to be rescaled.","metadata":{}},{"cell_type":"code","source":"#Copy the original dataframe to leave it intact\nkeypts = keypts_og.copy()\n\nx_old, y_old = original_image_size\nx_new, y_new = image_size_training\n\n#Get ratio of new to old for rescaling\nx_ratio = x_new / x_old\ny_ratio = y_new / y_old\n\nkeypts_x = ['lefteye_x', 'righteye_x', 'nose_x', 'leftmouth_x', 'rightmouth_x']\nkeypts_y = ['lefteye_y', 'righteye_y', 'nose_y', 'leftmouth_y', 'rightmouth_y']\n\n#Rescale the keypoints\nkeypts[keypts_x] = (keypts[keypts_x] * x_ratio)\nkeypts[keypts_y] = (keypts[keypts_y] * y_ratio)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:48.974518Z","iopub.execute_input":"2021-09-24T02:53:48.974871Z","iopub.status.idle":"2021-09-24T02:53:49.004763Z","shell.execute_reply.started":"2021-09-24T02:53:48.974835Z","shell.execute_reply":"2021-09-24T02:53:49.004048Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Check keypoints to see if rescaled, compare with original\nkeypts_og.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:49.005942Z","iopub.execute_input":"2021-09-24T02:53:49.006300Z","iopub.status.idle":"2021-09-24T02:53:49.021229Z","shell.execute_reply.started":"2021-09-24T02:53:49.006273Z","shell.execute_reply":"2021-09-24T02:53:49.020416Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"keypts.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:49.024017Z","iopub.execute_input":"2021-09-24T02:53:49.024279Z","iopub.status.idle":"2021-09-24T02:53:49.038367Z","shell.execute_reply.started":"2021-09-24T02:53:49.024253Z","shell.execute_reply":"2021-09-24T02:53:49.037541Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"keypts.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:49.039559Z","iopub.execute_input":"2021-09-24T02:53:49.040100Z","iopub.status.idle":"2021-09-24T02:53:49.091698Z","shell.execute_reply.started":"2021-09-24T02:53:49.040061Z","shell.execute_reply":"2021-09-24T02:53:49.091034Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Split data into training and test datasets","metadata":{}},{"cell_type":"code","source":"#Seaparate 20% for the test set\ntest_size = int(img_data_size * 0.2)\n\n# training data\ntrain_labels = keypts[:img_data_size - test_size]\ntrain_images = images[:img_data_size - test_size]\n\n\n# test data\ntest_labels = keypts[img_data_size - test_size + 1:]\ntest_images = images[img_data_size - test_size + 1:]","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:49.092891Z","iopub.execute_input":"2021-09-24T02:53:49.093277Z","iopub.status.idle":"2021-09-24T02:53:49.100199Z","shell.execute_reply.started":"2021-09-24T02:53:49.093226Z","shell.execute_reply":"2021-09-24T02:53:49.099410Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Train and Validation Sets","metadata":{}},{"cell_type":"code","source":"#Dropping image_id column\ny = train_labels.drop(['image_id'], axis = 1) \nX = train_images\n\n# check\ny.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:49.101911Z","iopub.execute_input":"2021-09-24T02:53:49.102354Z","iopub.status.idle":"2021-09-24T02:53:49.120344Z","shell.execute_reply.started":"2021-09-24T02:53:49.102316Z","shell.execute_reply":"2021-09-24T02:53:49.119203Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,random_state = 42)\n\n#Check the ratio\nX_val.shape[0]/X_train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:49.125676Z","iopub.execute_input":"2021-09-24T02:53:49.125930Z","iopub.status.idle":"2021-09-24T02:53:49.900065Z","shell.execute_reply.started":"2021-09-24T02:53:49.125906Z","shell.execute_reply":"2021-09-24T02:53:49.899324Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Building and Training the Model","metadata":{}},{"cell_type":"code","source":"#Img dimensions\nx_ = image_size_training[0]\ny_ = image_size_training[1]\n\n#Building the model\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=8, kernel_size=(3,3), padding='same', activation='relu', input_shape=(y_, x_, 3)))\nmodel.add(Conv2D(filters=8, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(10, activation='relu'))","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:49.902409Z","iopub.execute_input":"2021-09-24T02:53:49.902900Z","iopub.status.idle":"2021-09-24T02:53:51.659801Z","shell.execute_reply.started":"2021-09-24T02:53:49.902857Z","shell.execute_reply":"2021-09-24T02:53:51.659009Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:51.661037Z","iopub.execute_input":"2021-09-24T02:53:51.661359Z","iopub.status.idle":"2021-09-24T02:53:51.679656Z","shell.execute_reply.started":"2021-09-24T02:53:51.661326Z","shell.execute_reply":"2021-09-24T02:53:51.678925Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#Compile model\nmodel.compile(optimizer='adam', loss='mse', metrics=['mse'])","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:51.680662Z","iopub.execute_input":"2021-09-24T02:53:51.681013Z","iopub.status.idle":"2021-09-24T02:53:51.696492Z","shell.execute_reply.started":"2021-09-24T02:53:51.680970Z","shell.execute_reply":"2021-09-24T02:53:51.695214Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#Fit model \ntraining_process = model.fit(X_train, y_train, epochs=300, validation_data=(X_val, y_val), batch_size=200, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T02:53:51.697844Z","iopub.execute_input":"2021-09-24T02:53:51.698197Z","iopub.status.idle":"2021-09-24T03:06:06.037780Z","shell.execute_reply.started":"2021-09-24T02:53:51.698162Z","shell.execute_reply":"2021-09-24T03:06:06.035871Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Load trained model from previous commits to save time\n#model_path = '../input/80by35k/model80_35k'\n#model = keras.models.load_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:06:06.039905Z","iopub.execute_input":"2021-09-24T03:06:06.040298Z","iopub.status.idle":"2021-09-24T03:06:06.043701Z","shell.execute_reply.started":"2021-09-24T03:06:06.040258Z","shell.execute_reply":"2021-09-24T03:06:06.042801Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Check the loss and accuracy from loaded model to make sure it's same as last time\n#loss, mse = model.evaluate(test_images, test_labels.drop(['image_id'], axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:06:06.044907Z","iopub.execute_input":"2021-09-24T03:06:06.045650Z","iopub.status.idle":"2021-09-24T03:06:06.058125Z","shell.execute_reply.started":"2021-09-24T03:06:06.045606Z","shell.execute_reply":"2021-09-24T03:06:06.057300Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Saving the Model","metadata":{}},{"cell_type":"code","source":"#Saving the model as SavedModel format\nmodel.save('model80_35k')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:06:06.059400Z","iopub.execute_input":"2021-09-24T03:06:06.059881Z","iopub.status.idle":"2021-09-24T03:06:08.867282Z","shell.execute_reply.started":"2021-09-24T03:06:06.059845Z","shell.execute_reply":"2021-09-24T03:06:08.866341Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#Checking kaggle directory for the file\nimport os\nos.chdir(r'/kaggle/working')\nos.listdir()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:06:08.872625Z","iopub.execute_input":"2021-09-24T03:06:08.872904Z","iopub.status.idle":"2021-09-24T03:06:08.878713Z","shell.execute_reply.started":"2021-09-24T03:06:08.872878Z","shell.execute_reply":"2021-09-24T03:06:08.877827Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#Obtaining link to download files\nfrom IPython.display import FileLinks\nFileLinks(r'model80_35k')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:06:08.880227Z","iopub.execute_input":"2021-09-24T03:06:08.880912Z","iopub.status.idle":"2021-09-24T03:06:08.889510Z","shell.execute_reply.started":"2021-09-24T03:06:08.880872Z","shell.execute_reply":"2021-09-24T03:06:08.888591Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Testing the Model","metadata":{}},{"cell_type":"code","source":"# function to plot the image with green box around the faces\ndef plot_test_img(index, model, pred_or_actual = 'pred', pointsColor='ro-', boxcolor='g'):\n    img = tf.keras.preprocessing.image.load_img(\"{}/0{}.jpg\".format(images_data_path, index),target_size=(y_og,x_og,3))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    test_image = img/255\n    model = model\n    \n    #Obtain predictions for key points of face\n    if pred_or_actual == 'pred':\n        img = tf.keras.preprocessing.image.load_img(\"{}/0{}.jpg\".format(images_data_path, index),target_size=(y_,x_,3))\n        img = tf.keras.preprocessing.image.img_to_array(img)\n        img = img/255\n      \n        points_list = model.predict(img.reshape(1,y_,x_,3)).astype('int')[0]\n        \n        #convert key pts values to original size\n        x_ratio = 1.05 * (original_image_size[0] / image_size_training[0])\n        y_ratio = 1.085 * (original_image_size[1] / image_size_training[1])\n        \n        points_list[0] = int(points_list[0] * x_ratio)\n        points_list[2] = int(points_list[2] * x_ratio)\n        points_list[4] = int(points_list[4] * x_ratio)\n        points_list[6] = int(points_list[6] * x_ratio)\n        points_list[8] = int(points_list[8] * x_ratio)\n    \n        points_list[1] = int(points_list[1] * y_ratio)\n        points_list[3] = int(points_list[3] * y_ratio)\n        points_list[5] = int(points_list[5] * y_ratio)\n        points_list[7] = int(points_list[7] * y_ratio)\n        points_list[9] = int(points_list[9] * y_ratio)\n        \n        title = 'Predicted'\n        \n    elif pred_or_actual == 'actual':      \n        points_list = img_keypts_list(index)\n        pointsColor = 'bo-'\n        title = 'Actual'\n    \n    # face points\n    le_x, le_y, re_x, re_y = points_list[0], points_list[1], points_list[2], points_list[3]\n    n_x, n_y = points_list[4], points_list[5]\n    lm_x, lm_y, rm_x, rm_y = points_list[6], points_list[7], points_list[8], points_list[9]\n\n    # Create figure and axes\n    fig, ax = plt.subplots()\n    # plot the image\n    ax.imshow(test_image)\n    # plot the points on the face\n    ax.plot([le_x,re_x,n_x,lm_x,rm_x], [le_y,re_y,n_y,lm_y,rm_y], pointsColor)\n    ax.set_title(title)\n    \n    # plot the box around the face\n    width = abs(le_x-rm_x-60)\n    height = abs(le_y-rm_y-75)\n    rect = patches.Rectangle((le_x-30, le_y-40), width, height, linewidth=4, edgecolor=boxcolor, facecolor='none')\n    ax.add_patch(rect);\n    return points_list","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:15:53.269086Z","iopub.execute_input":"2021-09-24T03:15:53.269422Z","iopub.status.idle":"2021-09-24T03:15:53.287213Z","shell.execute_reply.started":"2021-09-24T03:15:53.269394Z","shell.execute_reply":"2021-09-24T03:15:53.285373Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"index = 14405\nplot_test_img(index, model, pred_or_actual = 'pred')\nplot_test_img(index, model, pred_or_actual='actual')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:15:31.380254Z","iopub.execute_input":"2021-09-24T03:15:31.380589Z","iopub.status.idle":"2021-09-24T03:15:31.739695Z","shell.execute_reply.started":"2021-09-24T03:15:31.380558Z","shell.execute_reply":"2021-09-24T03:15:31.738924Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation of Model","metadata":{}},{"cell_type":"code","source":"# losses of both training and validation sets\nloss = training_process.history['loss']\nval_loss = training_process.history['val_loss']\n\n# plot both losses\nplt.plot(loss)\nplt.plot(val_loss)\nplt.legend(['loss', 'val_loss']);","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:06:09.444934Z","iopub.execute_input":"2021-09-24T03:06:09.445303Z","iopub.status.idle":"2021-09-24T03:06:09.599142Z","shell.execute_reply.started":"2021-09-24T03:06:09.445267Z","shell.execute_reply":"2021-09-24T03:06:09.598173Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"I'd also like to evaluate the model MSE per each coordinate.","metadata":{}},{"cell_type":"code","source":"#Evaluation function to calculate MSE for each coordinate\ndef eval_model(labels, predictions, as_df=False):\n    \"\"\"Calculate the MSE for each for ten coordinates, plus an average of all the MSEs, with the option to output as a DataFrame or a list.\"\"\"\n    mse_list = []\n    \n    #Calculate each MSE and add to list\n    for i in np.arange(10):\n        if (type(labels)==pd.DataFrame):\n            mse_list.append(mean_squared_error(labels.iloc[:,i], [coord[i] for coord in predictions]))\n        else:\n            mse_list.append(mean_squared_error([label[i] for label in labels], [coord[i] for coord in predictions]))\n    \n    #Calculate average MSE\n    mse_list.append(np.mean(mse_list))\n    \n    #Optional output as dataframe\n    if (as_df):\n        cols = keypts_og.columns[1:].tolist()\n        cols.append('average')\n        \n        df = pd.DataFrame(columns=cols)\n        df.loc['MSE'] = mse_list\n        \n        return df\n    #Otherwise output as list\n    else: return mse_list","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:06:09.600542Z","iopub.execute_input":"2021-09-24T03:06:09.600897Z","iopub.status.idle":"2021-09-24T03:06:09.608498Z","shell.execute_reply.started":"2021-09-24T03:06:09.600858Z","shell.execute_reply":"2021-09-24T03:06:09.607580Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"test_lbs = test_labels.drop(['image_id'], axis=1)\n\nevaluate = eval_model(test_lbs, model.predict(test_images), as_df=True)\nevaluate","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:24:03.281893Z","iopub.execute_input":"2021-09-24T03:24:03.282266Z","iopub.status.idle":"2021-09-24T03:24:04.398432Z","shell.execute_reply.started":"2021-09-24T03:24:03.282232Z","shell.execute_reply":"2021-09-24T03:24:04.397631Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"Saving this information in a more accessible .csv file for comparison between models:","metadata":{}},{"cell_type":"code","source":"exp = pd.DataFrame(data={'loss': [training_process.history['loss']], \n                         'val_loss': [training_process.history['val_loss']],\n                         'epoch': 300,\n                         'batch': 200,\n                        },\n                  columns = ['loss', 'val_loss', 'epoch', 'batch'])\nexp.rename(index={0: 'Custom'}, inplace=True)\nexp","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:22:46.871617Z","iopub.execute_input":"2021-09-24T03:22:46.871964Z","iopub.status.idle":"2021-09-24T03:22:46.890427Z","shell.execute_reply.started":"2021-09-24T03:22:46.871932Z","shell.execute_reply":"2021-09-24T03:22:46.889411Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"#Add MSE data\nexp = pd.concat([exp, evaluate.rename(index={'MSE':'Custom'})], axis=1)\nexp","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:29:14.590522Z","iopub.execute_input":"2021-09-24T03:29:14.590915Z","iopub.status.idle":"2021-09-24T03:29:14.614735Z","shell.execute_reply.started":"2021-09-24T03:29:14.590882Z","shell.execute_reply":"2021-09-24T03:29:14.613736Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"#Save dataframe as .csv\nexp.to_csv('model_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:29:57.308201Z","iopub.execute_input":"2021-09-24T03:29:57.308547Z","iopub.status.idle":"2021-09-24T03:29:57.321943Z","shell.execute_reply.started":"2021-09-24T03:29:57.308509Z","shell.execute_reply":"2021-09-24T03:29:57.321196Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"#Download csv\nfrom IPython.display import FileLink\nFileLink(r'model_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:30:54.387849Z","iopub.execute_input":"2021-09-24T03:30:54.388216Z","iopub.status.idle":"2021-09-24T03:30:54.394140Z","shell.execute_reply.started":"2021-09-24T03:30:54.388183Z","shell.execute_reply":"2021-09-24T03:30:54.393145Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"Comparing the distribution of the predicted vs. actual keypoints:\n\n**Currently commented out to save RAM**","metadata":{}},{"cell_type":"code","source":"# predictions = model.predict(images.reshape(-1,y_,x_,3)).astype('int')\n# predictions.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Obtain keypoint names\n# cols = list(keypts.columns)\n# cols.remove('image_id')\n\n# #Convert predictions to dataframe\n# pred_df = pd.DataFrame(predictions, columns=cols)\n# print(pred_df.shape)\n# pred_df.sample(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Plotting distribution of the keypoints' coordinates\n# fig, axs = plt.subplots(5, 2, figsize=(13,10))\n\n# #Left eye\n# axs[0,0].hist(pred_df.lefteye_x, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.7)\n# axs[0,0].hist(keypts.lefteye_x, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[0,0].set_title('Left Eye X-coord')\n# axs[0,1].hist(pred_df.lefteye_y, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.5)\n# axs[0,1].hist(keypts.lefteye_y, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[0,1].set_title('Left Eye Y-coord')\n\n# #Right eye\n# axs[1,0].hist(pred_df.righteye_x, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.9)\n# axs[1,0].hist(keypts.righteye_x, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[1,0].set_title('Right Eye X-coord')\n# axs[1,1].hist(pred_df.righteye_y, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.5)\n# axs[1,1].hist(keypts.righteye_y, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[1,1].set_title('Right Eye Y-coord')\n\n# #Nose\n# axs[2,0].hist(pred_df.nose_x, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.9)\n# axs[2,0].hist(keypts.nose_x, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[2,0].set_title('Nose X-coord')\n# axs[2,1].hist(pred_df.nose_y, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.9)\n# axs[2,1].hist(keypts.nose_y, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[2,1].set_title('Nose Y-coord')\n\n# #Left mouth\n# axs[3,0].hist(pred_df.leftmouth_x, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.5)\n# axs[3,0].hist(keypts.leftmouth_x, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[3,0].set_title('Left Mouth X-coord')\n# axs[3,1].hist(pred_df.leftmouth_y, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.3)\n# axs[3,1].hist(keypts.leftmouth_y, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[3,1].set_title('Left Mouth Y-coord')\n\n# #Right mouth\n# axs[4,0].hist(pred_df.rightmouth_x, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.7)\n# axs[4,0].hist(keypts.rightmouth_x, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[4,0].set_title('Right Mouth X-coord')\n# axs[4,1].hist(pred_df.rightmouth_y, bins=15, alpha=0.5, color='blue', label='Predicted', width=0.5)\n# axs[4,1].hist(keypts.rightmouth_y, bins=15, alpha=0.5, color='red', label='Actual')\n# axs[4,1].set_title('Right Mouth Y-coord')\n\n# #Format legend\n# lines_labels = [axs[0,0].get_legend_handles_labels()]\n# lines, labels = [sum(i, []) for i in zip(*lines_labels)]\n# fig.legend(lines, labels, loc='upper center', ncol=2)\n\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of MSE","metadata":{}},{"cell_type":"code","source":"# #Obtain data and their actual keypoint values\n# im2 = images.copy()\n# actual2 = np.asarray(keypts.drop(['image_id'], axis=1)).transpose().reshape(-1,10).astype(np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mse_dist = []\n\n# #Collect MSEs for dataset\n# for data in range(im2.shape[0]):\n#     loss, mse = model.evaluate(im2[data].reshape(-1, x_, y_, 3), actual2[data].reshape(-1, 10), verbose=0)\n#     mse_dist.append(mse)\n#     if data%5000 == 0:\n#         print(\"Currently at {}/{} for MSE collection.\".format(data, im2.shape[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Plot the MSE Distribution\n# plt.hist(mse_dist)\n# plt.title('MSE Distribution')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Plot RMSE Distribution for easy interpretability\n# rmse_dist = np.sqrt(mse_dist)\n\n# plt.hist(rmse_dist)\n# plt.title('RMSE Distribution')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explainability of the model\n\nInstead of blindly trusting the 'black box,' I'm going to be delving further into what is going on under the hood to make sure there's some explainability to the model.","metadata":{}},{"cell_type":"code","source":"#Take a random image from the dataset\nIDX = 14404\n\npath = \"{}/{}\".format(str(images_data_path), str(keypts_og.iloc[IDX].image_id))\nim = PIL.Image.open(path).resize(image_size_training)\nim_array = np.asarray(im)\n\nim = im_array.reshape(-1, y_, x_, 3)\nim.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:14:25.624408Z","iopub.execute_input":"2021-09-24T03:14:25.624748Z","iopub.status.idle":"2021-09-24T03:14:25.639641Z","shell.execute_reply.started":"2021-09-24T03:14:25.624715Z","shell.execute_reply":"2021-09-24T03:14:25.638742Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(6,6*image_size_ratio))\nplt.imshow(im_array/255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:14:28.042955Z","iopub.execute_input":"2021-09-24T03:14:28.043311Z","iopub.status.idle":"2021-09-24T03:14:28.185703Z","shell.execute_reply.started":"2021-09-24T03:14:28.043279Z","shell.execute_reply":"2021-09-24T03:14:28.184849Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"#Check the loss and accuracy of the sample image\n#Obtain the actual coordinate values and reshape it as a tensor\nactual = np.asarray(keypts.iloc[IDX].drop(['image_id'])).transpose().reshape(-1,10).astype(np.float32)\n\neval_model(actual, model.predict(im), as_df=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:14:34.767762Z","iopub.execute_input":"2021-09-24T03:14:34.768127Z","iopub.status.idle":"2021-09-24T03:14:34.827981Z","shell.execute_reply.started":"2021-09-24T03:14:34.768093Z","shell.execute_reply":"2021-09-24T03:14:34.827192Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"#Visualize GradCAM output from 1st conv layer for left eye y-coord and nose x-coord\nexplainer = GradCAM()\n\ngrid1 = explainer.explain((im, None), model, 1, 'conv2d')\ngrid2 = explainer.explain((im, None), model, 4, 'conv2d')\n\n#Plot the visualizations\nfig = plt.figure(figsize = (18, 8))\n\nax1 = fig.add_subplot(1, 3, 1)\nax1.imshow(im_array / 255.)\nax1.imshow(grid1, alpha=1)\n\nax2 = fig.add_subplot(1, 3, 2)\nax2.imshow(im_array / 255.)\nax2.imshow(grid2, alpha=1)\n\nax3 = fig.add_subplot(1, 3, 3)\nax3.imshow(im_array / 255.)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:14:44.411186Z","iopub.execute_input":"2021-09-24T03:14:44.411521Z","iopub.status.idle":"2021-09-24T03:14:45.294070Z","shell.execute_reply.started":"2021-09-24T03:14:44.411491Z","shell.execute_reply":"2021-09-24T03:14:45.293068Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"The model is looking at the edges and outlines of the shapes, which makes sense since it's looking at one of its early layers. Going deeper into the CNN model, however, gives more model explainability.","metadata":{}},{"cell_type":"code","source":"#Visualize GradCAM output from 4th conv layer\ngrid1 = explainer.explain((im, None), model, 1, 'conv2d_4')\ngrid2 = explainer.explain((im, None), model, 4, 'conv2d_4')\n\n#Plot the visualizations\nfig = plt.figure(figsize = (18, 8))\n\nax1 = fig.add_subplot(1, 3, 1)\nax1.imshow(im_array / 255.)\nax1.imshow(grid1, alpha=1)\n\nax2 = fig.add_subplot(1, 3, 2)\nax2.imshow(im_array / 255.)\nax2.imshow(grid2, alpha=1)\n\nax3 = fig.add_subplot(1, 3, 3)\nax3.imshow(im_array / 255.)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:14:49.725853Z","iopub.execute_input":"2021-09-24T03:14:49.726240Z","iopub.status.idle":"2021-09-24T03:14:50.238530Z","shell.execute_reply.started":"2021-09-24T03:14:49.726206Z","shell.execute_reply":"2021-09-24T03:14:50.237719Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"#Visualize GradCAM output from last conv layer\ngrid1 = explainer.explain((im, None), model, 1, 'conv2d_6')\ngrid2 = explainer.explain((im, None), model, 4, 'conv2d_6')\n\n#Plot the visualizations\nfig = plt.figure(figsize = (18, 8))\n\nax1 = fig.add_subplot(1, 3, 1)\nax1.imshow(im_array / 255.)\nax1.imshow(grid1, alpha=1)\n\nax2 = fig.add_subplot(1, 3, 2)\nax2.imshow(im_array / 255.)\nax2.imshow(grid2, alpha=1)\n\nax3 = fig.add_subplot(1, 3, 3)\nax3.imshow(im_array / 255.)","metadata":{"execution":{"iopub.status.busy":"2021-09-24T03:14:54.872891Z","iopub.execute_input":"2021-09-24T03:14:54.873272Z","iopub.status.idle":"2021-09-24T03:14:55.337798Z","shell.execute_reply.started":"2021-09-24T03:14:54.873237Z","shell.execute_reply":"2021-09-24T03:14:55.336942Z"},"trusted":true},"execution_count":79,"outputs":[]}]}